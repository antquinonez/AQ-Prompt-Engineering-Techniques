{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code, Discover, Improve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note 1) Using ChatGPT4. Tried it with 3.5, and the code generation from the first ask was not usable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note 2) This is not an executable Jupyter Notebook. Just follow along."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: We'll demonstrate how to get started with code, do needed exploration, then patch the original code with what we discover. The imortant part: This all happens in ChatGPT. We won't be manually updating the code we start with. ChatGPT does the patching for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Steps:\n",
    "1) We'll put together a set of instructions and ask chatGPT for code.\n",
    "2) Find out the code is not good enough.\n",
    "3) Do some more asking.\n",
    "4) Finish with a telling for a code update."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### ASK THIS of CHATGPT. Keep the hashes in the request. Don't be at all concerned with the purpose of the code or whether it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a python program with a main function\n",
    "# the main function will be called by the cloud function\n",
    "# the main function will take a pubsub message as an argument\n",
    "# the main program will extract the project, data-set, and table from landed table from the pubsub message\n",
    "# the main function will print the pubsub message\n",
    "# the main function will return a string\n",
    "# when successful, the cloud function will return the string\n",
    "# when unsuccessful, the cloud function will return an error message"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You'll get something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "def main(event, context):\n",
    "    try:\n",
    "        # Get the Pub/Sub message from the event\n",
    "        pubsub_message = event['data']\n",
    "        \n",
    "        # Decode the message from base64\n",
    "        decoded_message = base64.b64decode(pubsub_message).decode('utf-8')\n",
    "        \n",
    "        # Load the decoded message as a JSON object\n",
    "        message_json = json.loads(decoded_message)\n",
    "        \n",
    "        # Extract the project, data-set, and table from the JSON object\n",
    "        project = message_json.get('project', '')\n",
    "        data_set = message_json.get('data_set', '')\n",
    "        table = message_json.get('table', '')\n",
    "        \n",
    "        # Print the extracted information\n",
    "        print(f\"Pub/Sub message: {decoded_message}\")\n",
    "        print(f\"Project: {project}\")\n",
    "        print(f\"Data Set: {data_set}\")\n",
    "        print(f\"Table: {table}\")\n",
    "        \n",
    "        # Return a success string\n",
    "        return f\"Successfully processed message: {decoded_message}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Return an error message if there's an issue\n",
    "        return f\"Error processing message: {str(e)}\"\n",
    "\n",
    "# If this file is being run as a script, call the main function with sample data\n",
    "if __name__ == \"__main__\":\n",
    "    sample_event = {\n",
    "        'data': base64.b64encode(json.dumps({\n",
    "            'project': 'sample_project',\n",
    "            'data_set': 'sample_data_set',\n",
    "            'table': 'sample_table'\n",
    "        }).encode('utf-8')).decode('utf-8')\n",
    "    }\n",
    "    sample_context = None\n",
    "    result = main(sample_event, sample_context)\n",
    "    print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: This code snippet is not good enough because the \"Extract the project, data-set, and table from the JSON object\" won't actually get me the table related info from the event data. It's my fault. Let's pretend that I didn't know to tell ChatGPT that the event was for the creation of a table in BigQuery. in GCP, when you create a table and listen out for it's creation (details aren't important), you can work with the JSON telling you all about that table creation. Let's do some more asking."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask ChatGP:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a bigquery json field look like when a table is created in a particular data-set?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You'll get something like this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"protoPayload\": {\n",
    "    \"@type\": \"type.googleapis.com/google.cloud.audit.AuditLog\",\n",
    "    \"status\": {},\n",
    "    \"authenticationInfo\": {\n",
    "      \"principalEmail\": \"user@example.com\"\n",
    "    },\n",
    "    \"requestMetadata\": {\n",
    "      \"callerIp\": \"1.2.3.4\",\n",
    "      \"callerSuppliedUserAgent\": \"google-cloud-sdk\",\n",
    "      \"requestAttributes\": {},\n",
    "      \"destinationAttributes\": {}\n",
    "    },\n",
    "    \"serviceName\": \"bigquery.googleapis.com\",\n",
    "    \"methodName\": \"google.cloud.bigquery.v2.TableService.InsertTable\",\n",
    "    \"authorizationInfo\": [\n",
    "      {\n",
    "        \"permission\": \"bigquery.tables.create\",\n",
    "        \"granted\": true,\n",
    "        \"resource\": \"projects/sample-project/datasets/sample_dataset\"\n",
    "      }\n",
    "    ],\n",
    "    \"resourceName\": \"projects/sample-project/datasets/sample_dataset/tables/sample_table\",\n",
    "    \"request\": {\n",
    "      \"tableReference\": {\n",
    "        \"projectId\": \"sample-project\",\n",
    "        \"datasetId\": \"sample_dataset\",\n",
    "        \"tableId\": \"sample_table\"\n",
    "      },\n",
    "      \"schema\": {\n",
    "        \"fields\": [\n",
    "          {\n",
    "            \"name\": \"id\",\n",
    "            \"type\": \"INTEGER\",\n",
    "            \"mode\": \"NULLABLE\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"name\",\n",
    "            \"type\": \"STRING\",\n",
    "            \"mode\": \"NULLABLE\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"timePartitioning\": {\n",
    "        \"type\": \"DAY\"\n",
    "      },\n",
    "      \"description\": \"Sample table description\",\n",
    "      \"friendlyName\": \"Sample Table\"\n",
    "    },\n",
    "    \"response\": {\n",
    "      \"@type\": \"type.googleapis.com/google.cloud.bigquery.v2.Table\",\n",
    "      \"tableReference\": {\n",
    "        \"projectId\": \"sample-project\",\n",
    "        \"datasetId\": \"sample_dataset\",\n",
    "        \"tableId\": \"sample_table\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"insertId\": \"abc123\",\n",
    "  \"resource\": {\n",
    "    \"type\": \"bigquery_resource\",\n",
    "    \"labels\": {\n",
    "      \"project_id\": \"sample-project\",\n",
    "      \"dataset_id\": \"sample_dataset\",\n",
    "      \"table_id\": \"sample_table\"\n",
    "    }\n",
    "  },\n",
    "  \"timestamp\": \"2021-10-01T00:00:00.000Z\",\n",
    "  \"severity\": \"NOTICE\",\n",
    "  \"logName\": \"projects/sample-project/logs/cloudaudit.googleapis.com%2Factivity\",\n",
    "  \"receiveTimestamp\": \"2021-10-01T00:00:00.000Z\"\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask ChatGPT:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With python, write code to extract the project, dataset, and table name from the above json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You'll get something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writer's note: I'll skip the assignment of some long JSON string to a json_string variable and continue with the important code...\n",
    "\n",
    "# Load the JSON string as a Python dictionary\n",
    "json_data = json.loads(json_string)\n",
    "\n",
    "# Extract the project, dataset, and table name from the JSON data\n",
    "project = json_data['protoPayload']['request']['tableReference']['projectId']\n",
    "dataset = json_data['protoPayload']['request']['tableReference']['datasetId']\n",
    "table = json_data['protoPayload']['request']['tableReference']['tableId']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cool Part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask CHATGPT:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code variable assignment code for project, dataset, and table you just generated to update the  program generated from the prompt with the #s."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You'll get this, with the patched code for extracting the data we need from the JSON!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "def main(event, context):\n",
    "    try:\n",
    "        # Get the Pub/Sub message from the event\n",
    "        pubsub_message = event['data']\n",
    "        \n",
    "        # Decode the message from base64\n",
    "        decoded_message = base64.b64decode(pubsub_message).decode('utf-8')\n",
    "        \n",
    "        # Load the decoded message as a JSON object\n",
    "        message_json = json.loads(decoded_message)\n",
    "\n",
    "        # Extract the project, dataset, and table name from the JSON data\n",
    "        project = message_json['protoPayload']['request']['tableReference']['projectId']\n",
    "        dataset = message_json['protoPayload']['request']['tableReference']['datasetId']\n",
    "        table = message_json['protoPayload']['request']['tableReference']['tableId']\n",
    "        \n",
    "        # Print the extracted information\n",
    "        print(f\"Pub/Sub message: {decoded_message}\")\n",
    "        print(f\"Project: {project}\")\n",
    "        print(f\"Dataset: {dataset}\")\n",
    "        print(f\"Table: {table}\")\n",
    "        \n",
    "        # Return a success string\n",
    "        return f\"Successfully processed message: {decoded_message}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Return an error message if there's an issue\n",
    "        return f\"Error processing message: {str(e)}\"\n",
    "\n",
    "# If this file is being run as a script, call the main function with sample data\n",
    "if __name__ == \"__main__\":\n",
    "    sample_event = {\n",
    "        'data': base64.b64encode(json.dumps({\n",
    "            'protoPayload': {\n",
    "                'request': {\n",
    "                    'tableReference': {\n",
    "                        'projectId': 'sample-project',\n",
    "                        'datasetId': 'sample_dataset',\n",
    "                        'tableId': 'sample_table'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }).encode('utf-8')).decode('utf-8')\n",
    "    }\n",
    "    sample_context = None\n",
    "    result = main(sample_event, sample_context)\n",
    "    print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I demonstrated a workflow that started with a not-quite complete description of what we needed and defintely did not setup ChatGPT for success. Incrementally, I worked myself towards a solution and then looped back to the orginal code request, referring to that code by the incidental usage of #s (which were from a Python Script where I was starting out with a comment block), and asked to patch the code with the new understanding of how to work with JSON that describes a table creation.\n",
    "\n",
    "Observations:\n",
    "1) The sample event in the '_main_' part was updated to reflect the relevant BigQuery JSON structure. I DID NOT DO THAT. ChatGPT did that.\n",
    "2) When I tried these instructions in a separate ChatGPT session, I noticed that the variable names for project, dataset, and table name were different in the original request, compared to the request for working with a sample table creation JSON. Yet, when I asked for the update of the original code with the updated fragment, it had no problem updating the appropriate variable assignments AND still keep the original variable names! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aqpe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
